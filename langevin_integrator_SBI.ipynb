{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import einops\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import bottleneck as bn\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.mamba import Mamba, MambaConfig\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from sbi.inference.base import infer\n",
    "from sbi.inference import SNPE, prepare_for_sbi, simulate_for_sbi\n",
    "from sbi import analysis, utils\n",
    "from sbi.inference import SNPE, simulate_for_sbi\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")\n",
    "# import required modules\n",
    "from sbi.utils.get_nn_models import posterior_nn\n",
    "from sbi.neural_nets.embedding_nets import (\n",
    "    FCEmbedding,\n",
    "    CNNEmbedding,\n",
    "    PermutationInvariantEmbedding\n",
    ")\n",
    "seed = 0 \n",
    "torch.manual_seed(seed) \n",
    "from src.temporal_encoders import ResidualTemporalBlock, Residual, PreNorm, LinearAttention, Downsample1d, Conv1dBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, fastmath=True)\n",
    "def force(x, k=3):\n",
    "    Fx = -k * x\n",
    "    return Fx\n",
    "\n",
    "@nb.jit(nopython=True, fastmath=True)\n",
    "def langevin_integrator(\n",
    "    x0=-1.5,\n",
    "    vx0=0.0,\n",
    "    mx=1,\n",
    "    nux=1,\n",
    "    k=3,\n",
    "    N=1000,\n",
    "    dt=5e-5,\n",
    "    fs=1\n",
    "):\n",
    "    \n",
    "    beta = 1 # by default we set the temperature to 1 to simplify the calculation\n",
    "    invmass = 1.0 / mx\n",
    "    sigma = np.sqrt(2.0 * nux / (beta * mx))\n",
    "    b1 = 1.0 - 0.5 * dt * nux + 0.125 * (dt ** 2) * (nux ** 2)\n",
    "    b2 = 0.5 * dt - 0.125 * nux * dt ** 2\n",
    "    s3 = np.sqrt(3.0)\n",
    "    sdt3 = sigma * np.sqrt(dt ** 3)\n",
    "    sdt = sigma * np.sqrt(dt)\n",
    "\n",
    "    n = int(N / fs)\n",
    "    x = np.zeros(n)\n",
    "    xi = np.random.standard_normal(size=n)\n",
    "    eta = np.random.standard_normal(size=n)\n",
    "    x[0] = x0\n",
    "    vx = vx0\n",
    "    xold = x0\n",
    "    Fx = force(x0, k=k) * invmass\n",
    "\n",
    "    for i in range(1, N):\n",
    "\n",
    "        # Drawing noise \n",
    "        if i%n == 0:\n",
    "            xi = np.random.standard_normal(size=n)\n",
    "            eta = np.random.standard_normal(size=n)\n",
    "\n",
    "        _xi = xi[i%n]\n",
    "        _eta = eta[i%n]\n",
    "\n",
    "        n1 = 0.5 * sdt * _xi\n",
    "        n3 = 0.5 * sdt3 * _eta / s3\n",
    "        n4 = sdt3 * (0.125 * _xi + 0.25 * _eta / s3)\n",
    "        n5 = n1 - nux * n4\n",
    "\n",
    "        vx = vx * b1 + Fx * b2 + n5\n",
    "        xnew = xold + dt * vx + n3\n",
    "        \n",
    "        Fx = force(xnew, k=k) * invmass\n",
    "        vx = vx * b1 + Fx * b2 + n5\n",
    "        \n",
    "        if (i % fs) == 0:\n",
    "            x[int(i / fs)] = xnew\n",
    "\n",
    "        xold = xnew\n",
    "\n",
    "    return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Langevin_simulator_SSM(params):\n",
    "    params = np.array(params.cpu(), dtype=np.float64)\n",
    "    x = langevin_integrator(\n",
    "    x0=0,\n",
    "    vx0=0.0,\n",
    "    mx= 10**params[0],\n",
    "    nux=10**params[1],\n",
    "    k=3,\n",
    "    N=1000,\n",
    "    dt=5e-3,\n",
    "    fs=1\n",
    ")\n",
    "    return torch.tensor(x, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "def Langevin_simulator_Transformer(params):\n",
    "    params = np.array(params.cpu(), dtype=np.float64)\n",
    "    x = langevin_integrator(\n",
    "    x0=0,\n",
    "    vx0=0.0,\n",
    "    mx= 10**params[0],\n",
    "    nux=10**params[1],\n",
    "    k=3,\n",
    "    N=1000,\n",
    "    dt=5e-3,\n",
    "    fs=1\n",
    ")\n",
    "    return torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "def Langevin_simulator_Opt_CNN(params):\n",
    "    params = np.array(params.cpu(), dtype=np.float64)\n",
    "    x = langevin_integrator(\n",
    "    x0=0,\n",
    "    vx0=0.0,\n",
    "    mx= 10**params[0],\n",
    "    nux=10**params[1],\n",
    "    k=3,\n",
    "    N=1000,\n",
    "    dt=5e-3,\n",
    "    fs=1\n",
    ")\n",
    "    return torch.tensor(x, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, transition_dim, dim=32, kernel_sizes=(4, 4), stride=2, num_heads=4, depth=4, mlp_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=transition_dim if i == 0 else dim, out_channels=dim, \n",
    "                      kernel_size=ks, stride=stride, padding=ks//2)\n",
    "            for i, ks in enumerate(kernel_sizes)\n",
    "        ])\n",
    "\n",
    "        self.pos_embedding = PositionalEncoding(dim, dropout=0.1, max_len=5000)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        \n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=mlp_dim,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=depth)\n",
    "\n",
    "        self.to_out = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            x : [ batch x horizon x transition ]\n",
    "        '''\n",
    "\n",
    "        x= x.unsqueeze(2)\n",
    "        x = einops.rearrange(x, 'b h t -> b t h')\n",
    "\n",
    "        for conv in self.conv_layers:\n",
    "            x = F.relu(conv(x))\n",
    "\n",
    "        x = einops.rearrange(x, 'b t h -> b h t')\n",
    "        x = self.pos_embedding(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1) \n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # Shape: [batch, horizon + 1, dim]\n",
    "        \n",
    "        x = self.transformer(x)  \n",
    "\n",
    "        x = x[:, 0]\n",
    "        x = self.to_out(x)\n",
    "        return x\n",
    "    \n",
    "#Optional Attention\n",
    "class TemporalCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transition_dim,\n",
    "        dim=32,\n",
    "        dim_mults=(1, 2, 4),\n",
    "        attention=True,\n",
    "        padding_mode='reflect',\n",
    "        kernel_size=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        dims = [transition_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "        print(f'[ models/temporal ] Channel dimensions: {dims}')\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                ResidualTemporalBlock(dim_in, dim_out, kernel_size = kernel_size, padding_mode=padding_mode),\n",
    "                ResidualTemporalBlock(dim_out, dim_out, kernel_size = kernel_size, padding_mode=padding_mode),\n",
    "                Residual(PreNorm(dim_out, LinearAttention(dim_out))) if attention else nn.Identity(),\n",
    "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = ResidualTemporalBlock(mid_dim, mid_dim, kernel_size = kernel_size, padding_mode=padding_mode)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, LinearAttention(mid_dim))) if attention else nn.Identity()\n",
    "        self.mid_block2 = ResidualTemporalBlock(mid_dim, mid_dim, kernel_size = kernel_size, padding_mode=padding_mode)\n",
    "\n",
    "        self.proj_out = nn.Linear(mid_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            x : [ batch x horizon x transition ]\n",
    "        '''\n",
    "\n",
    "        #reshape to [batch x transition x horizon]\n",
    "\n",
    "        x = einops.rearrange(x, 'b h t -> b t h')\n",
    "\n",
    "\n",
    "        for resnet, resnet2, attn, downsample in self.downs:\n",
    "            x = resnet(x)\n",
    "            x = resnet2(x)\n",
    "            x = attn(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x)\n",
    "        x = x.mean(dim=-1)\n",
    "        x = self.proj_out(x)\n",
    "        return x\n",
    "\n",
    "#SSM\n",
    "class TemporalMamba(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transition_dim,\n",
    "        dim=128,\n",
    "        kernel_size=3,\n",
    "        expand=2,\n",
    "        num_layers=1, \n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mamba_layers = nn.ModuleList()\n",
    "        self.l_norm_layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            config = MambaConfig(n_layers=num_layers, d_model=dim, d_state=dim, d_conv=kernel_size, expand_factor=expand)\n",
    "            self.mamba_layers.append(Mamba(config)\n",
    "            )\n",
    "        \n",
    "            self.l_norm_layers.append(nn.LayerNorm(dim))\n",
    "        \n",
    "        self.x_emb = nn.Linear(transition_dim, dim)\n",
    "        self.proj_out = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            x : [ batch x horizon x transition ]\n",
    "        '''\n",
    "        x = self.x_emb(x)\n",
    "        for mamba, l_norm in zip(self.mamba_layers, self.l_norm_layers):\n",
    "            x_in = x\n",
    "            x = mamba(x)\n",
    "            x = l_norm(x + x_in)\n",
    "\n",
    "        x = x[:, -1]\n",
    "        x = self.proj_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalMamba(transition_dim=1, dim=32, kernel_size=3, expand=2, num_layers=2) #SSM\n",
    "#model = TemporalCNN(1, dim=32, dim_mults=(1, 2, 4), attention=True, padding_mode='reflect', kernel_size=1) #Opt CNN\n",
    "#model = TemporalTransformer(transition_dim=1, dim=32, kernel_sizes=(4, 4), stride=2, num_heads=4, depth=4, mlp_dim=32) # Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nux_limits = [-1, 2]\n",
    "mass_limits = [-1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = utils.BoxUniform(\n",
    "    low = torch.tensor([mass_limits[0],nux_limits[0]], device='cuda'),\n",
    "    high = torch.tensor([mass_limits[1],nux_limits[1]], device='cuda')\n",
    ")\n",
    "\n",
    "prior, num_parameters, prior_returns_numpy= process_prior(prior)\n",
    "\n",
    "simulator_wrapper = process_simulator(Langevin_simulator_SSM, prior, prior_returns_numpy)\n",
    "\n",
    "check_sbi_inputs(simulator_wrapper, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Langevin_simulator_SSM, prior = prepare_for_sbi(Langevin_simulator_SSM, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_posterior = posterior_nn(model='nsf', embedding_net = model)\n",
    "\n",
    "inference = SNPE(prior, device='cuda', density_estimator=neural_posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the inference procedure on one round and 10000 simulated data points\n",
    "theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=50000, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_estimator = inference.append_simulations(theta, x, data_device='cuda').train(training_batch_size=128, show_train_summary=True)\n",
    "\n",
    "posterior = inference.build_posterior(density_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('your_path.pkl', 'rb') as f:\n",
    "    posterior = torch.save(f) #save the posterior for later use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
